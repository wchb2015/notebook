{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379f0a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/23 07:30:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"test\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed8c6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filePath = \"\"\"/Users/chongbei/Music/LearningSparkV2/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet/\"\"\"\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\",\n",
    "                \"number_of_reviews\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a706c58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 07:30:39 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5780 rows in the training set, and 1366 in the test set\n"
     ]
    }
   ],
   "source": [
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42) \n",
    "print(f\"\"\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0cd1172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- cancellation_policy: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- host_total_listings_count: double (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: double (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bedrooms: double (nullable = true)\n",
      " |-- beds: double (nullable = true)\n",
      " |-- bed_type: string (nullable = true)\n",
      " |-- minimum_nights: double (nullable = true)\n",
      " |-- number_of_reviews: double (nullable = true)\n",
      " |-- review_scores_rating: double (nullable = true)\n",
      " |-- review_scores_accuracy: double (nullable = true)\n",
      " |-- review_scores_cleanliness: double (nullable = true)\n",
      " |-- review_scores_checkin: double (nullable = true)\n",
      " |-- review_scores_communication: double (nullable = true)\n",
      " |-- review_scores_location: double (nullable = true)\n",
      " |-- review_scores_value: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms_na: double (nullable = true)\n",
      " |-- bathrooms_na: double (nullable = true)\n",
      " |-- beds_na: double (nullable = true)\n",
      " |-- review_scores_rating_na: double (nullable = true)\n",
      " |-- review_scores_accuracy_na: double (nullable = true)\n",
      " |-- review_scores_cleanliness_na: double (nullable = true)\n",
      " |-- review_scores_checkin_na: double (nullable = true)\n",
      " |-- review_scores_communication_na: double (nullable = true)\n",
      " |-- review_scores_location_na: double (nullable = true)\n",
      " |-- review_scores_value_na: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airbnbDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db7739c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "|     1.0|   [1.0]|115.0|\n",
      "|     1.0|   [1.0]|105.0|\n",
      "|     1.0|   [1.0]| 86.0|\n",
      "|     1.0|   [1.0]|100.0|\n",
      "|     2.0|   [2.0]|220.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\") \n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select(\"bedrooms\", \"features\", \"price\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c87cfbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 07:30:48 WARN Instrumentation: [466f728b] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/02/23 07:30:48 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/02/23 07:30:48 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lrModel = lr.fit(vecTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dba0f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is price = 123.68*bedrooms + 47.51\n"
     ]
    }
   ],
   "source": [
    "m = round(lrModel.coefficients[0], 2)\n",
    "b = round(lrModel.intercept, 2)\n",
    "print(f\"\"\"The formula for the linear regression line is price = {m}*bedrooms + {b}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "032cf777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 07:30:53 WARN Instrumentation: [9767276e] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr]) \n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf39bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+------------------+\n",
      "|bedrooms|features| price|        prediction|\n",
      "+--------+--------+------+------------------+\n",
      "|     1.0|   [1.0]|  85.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  45.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  70.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 128.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 159.0|171.18598011578285|\n",
      "|     2.0|   [2.0]| 250.0|294.86172649777757|\n",
      "|     1.0|   [1.0]|  99.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  95.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 100.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|2010.0|171.18598011578285|\n",
      "+--------+--------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7bdb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols,\n",
    "                              outputCols=indexOutputCols,\n",
    "                              handleInvalid=\"skip\")\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n",
    "                           outputCols=oheOutputCols)\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes\n",
    "               if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs,\n",
    "                               outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0fc02b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "rFormula = RFormula(formula=\"price ~ .\",\n",
    "                    featuresCol=\"features\",\n",
    "                    labelCol=\"price\",\n",
    "                    handleInvalid=\"skip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2edcbc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 07:31:03 WARN Instrumentation: [9234e413] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,22,43,...| 85.0| 55.24365707389188|\n",
      "|(98,[0,3,6,22,43,...| 45.0|23.357685914717877|\n",
      "|(98,[0,3,6,22,43,...| 70.0|28.474464479034395|\n",
      "|(98,[0,3,6,12,42,...|128.0| -91.6079079594947|\n",
      "|(98,[0,3,6,12,43,...|159.0| 95.05688229945372|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[stringIndexer, oheEncoder, vecAssembler, lr])  # Or use RFormula\n",
    "# pipeline = Pipeline(stages = [rFormula, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "421a29bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 220.6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=\"price\",\n",
    "    metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c07ca930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.16043316698848087\n"
     ]
    }
   ],
   "source": [
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF) \n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3e1788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelinePath = \"/Users/chongbei/Music/test\"\n",
    "pipelineModel.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "912256d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel \n",
    "savedPipelineModel = PipelineModel.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8e0a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(labelCol=\"price\")\n",
    "# Filter for just numeric columns (and exclude price, our label)\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes\n",
    "           if ((dataType == \"double\") & (field != \"price\"))]\n",
    "# Combine output of StringIndexer defined above and numeric columns\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "# Combine stages into pipeline\n",
    "stages = [stringIndexer, vecAssembler, dt]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "#pipelineModel = pipeline.fit(trainDF) # This line should error\n",
    "dt.setMaxBins(40)\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c50198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_8135be4d67bd, depth=5, numNodes=47, numFeatures=33\n",
      "  If (feature 12 <= 2.5)\n",
      "   If (feature 12 <= 1.5)\n",
      "    If (feature 5 in {1.0,2.0})\n",
      "     If (feature 4 in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 104.23992784125075\n",
      "      Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 250.7111111111111\n",
      "     Else (feature 4 not in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 151.94179894179894\n",
      "      Else (feature 3 not in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 245.8507462686567\n",
      "    Else (feature 5 not in {1.0,2.0})\n",
      "     If (feature 3 in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 3 in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 131.96658097686375\n",
      "      Else (feature 3 not in {5.0,8.0,13.0,15.0,16.0,19.0,22.0,23.0,24.0,25.0,28.0,30.0,33.0})\n",
      "       Predict: 164.19959266802445\n",
      "     Else (feature 3 not in {1.0,5.0,6.0,7.0,8.0,9.0,11.0,13.0,15.0,16.0,17.0,19.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 10 <= 6.5)\n",
      "       Predict: 205.5814889336016\n",
      "      Else (feature 10 > 6.5)\n",
      "       Predict: 841.6666666666666\n",
      "   Else (feature 12 > 1.5)\n",
      "    If (feature 13 <= 4.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 26.5)\n",
      "       Predict: 290.8357933579336\n",
      "      Else (feature 14 > 26.5)\n",
      "       Predict: 214.04819277108433\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,15.0,16.0,17.0,18.0,19.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,33.0,34.0})\n",
      "      If (feature 14 <= 3.5)\n",
      "       Predict: 741.64\n",
      "      Else (feature 14 > 3.5)\n",
      "       Predict: 309.03921568627453\n",
      "    Else (feature 13 > 4.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      If (feature 2 in {1.0})\n",
      "       Predict: 300.0\n",
      "      Else (feature 2 not in {1.0})\n",
      "       Predict: 10000.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      If (feature 3 in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 222.91666666666666\n",
      "      Else (feature 3 not in {1.0,4.0,5.0,7.0,8.0,19.0})\n",
      "       Predict: 398.0\n",
      "  Else (feature 12 > 2.5)\n",
      "   If (feature 1 in {0.0,1.0,2.0,3.0,4.0})\n",
      "    If (feature 12 <= 5.5)\n",
      "     If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 14 <= 7.5)\n",
      "       Predict: 493.3795620437956\n",
      "      Else (feature 14 > 7.5)\n",
      "       Predict: 296.76666666666665\n",
      "     Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,10.0,11.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,21.0,22.0,23.0,24.0,25.0,26.0,28.0,29.0,30.0,33.0})\n",
      "      If (feature 9 <= -122.411075)\n",
      "       Predict: 722.96875\n",
      "      Else (feature 9 > -122.411075)\n",
      "       Predict: 2399.4\n",
      "    Else (feature 12 > 5.5)\n",
      "     If (feature 4 in {0.0,1.0,5.0,7.0})\n",
      "      If (feature 3 in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 609.5\n",
      "      Else (feature 3 not in {0.0,3.0,6.0,25.0})\n",
      "       Predict: 1715.0\n",
      "     Else (feature 4 not in {0.0,1.0,5.0,7.0})\n",
      "      Predict: 8000.0\n",
      "   Else (feature 1 not in {0.0,1.0,2.0,3.0,4.0})\n",
      "    Predict: 8000.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtModel = pipelineModel.stages[-1] \n",
    "print(dtModel.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "643b7512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.283406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancellation_policyIndex</td>\n",
       "      <td>0.167893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant_bookableIndex</td>\n",
       "      <td>0.140081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>property_typeIndex</td>\n",
       "      <td>0.128179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>number_of_reviews</td>\n",
       "      <td>0.126233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neighbourhood_cleansedIndex</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>longitude</td>\n",
       "      <td>0.038810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>minimum_nights</td>\n",
       "      <td>0.029473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>beds</td>\n",
       "      <td>0.015218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>room_typeIndex</td>\n",
       "      <td>0.010905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>accommodates</td>\n",
       "      <td>0.003603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>host_is_superhostIndex</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bathrooms_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beds_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>review_scores_rating_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>review_scores_accuracy_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>review_scores_cleanliness_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>review_scores_value</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>review_scores_checkin_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>review_scores_communication_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>review_scores_location_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bedrooms_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review_scores_rating</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>review_scores_location</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>review_scores_communication</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_scores_checkin</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>review_scores_cleanliness</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review_scores_accuracy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>latitude</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>host_total_listings_count</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bed_typeIndex</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>review_scores_value_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           feature  importance\n",
       "12                        bedrooms    0.283406\n",
       "1         cancellation_policyIndex    0.167893\n",
       "2            instant_bookableIndex    0.140081\n",
       "4               property_typeIndex    0.128179\n",
       "15               number_of_reviews    0.126233\n",
       "3      neighbourhood_cleansedIndex    0.056200\n",
       "9                        longitude    0.038810\n",
       "14                  minimum_nights    0.029473\n",
       "13                            beds    0.015218\n",
       "5                   room_typeIndex    0.010905\n",
       "10                    accommodates    0.003603\n",
       "0           host_is_superhostIndex    0.000000\n",
       "24                    bathrooms_na    0.000000\n",
       "25                         beds_na    0.000000\n",
       "26         review_scores_rating_na    0.000000\n",
       "27       review_scores_accuracy_na    0.000000\n",
       "28    review_scores_cleanliness_na    0.000000\n",
       "22             review_scores_value    0.000000\n",
       "29        review_scores_checkin_na    0.000000\n",
       "30  review_scores_communication_na    0.000000\n",
       "31       review_scores_location_na    0.000000\n",
       "23                     bedrooms_na    0.000000\n",
       "16            review_scores_rating    0.000000\n",
       "21          review_scores_location    0.000000\n",
       "20     review_scores_communication    0.000000\n",
       "19           review_scores_checkin    0.000000\n",
       "18       review_scores_cleanliness    0.000000\n",
       "17          review_scores_accuracy    0.000000\n",
       "11                       bathrooms    0.000000\n",
       "8                         latitude    0.000000\n",
       "7        host_total_listings_count    0.000000\n",
       "6                    bed_typeIndex    0.000000\n",
       "32          review_scores_value_na    0.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "featureImp = pd.DataFrame(\n",
    "      list(zip(vecAssembler.getInputCols(), dtModel.featureImportances)),\n",
    "columns=[\"feature\", \"importance\"])\n",
    "featureImp.sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7ecb020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8af9667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1095de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder \n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "            .addGrid(rf.numTrees, [10, 100])\n",
    "            .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2a8d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"price\",\n",
    "                                    predictionCol=\"prediction\",\n",
    "                                    metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56218163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5893db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/Users/chongbei/anaconda3/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.0.jar) to field java.nio.charset.Charset.name\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/02/23 08:41:27 WARN DAGScheduler: Broadcasting large task binary with size 1323.1 KiB\n",
      "24/02/23 08:41:33 WARN DAGScheduler: Broadcasting large task binary with size 1156.6 KiB\n",
      "24/02/23 08:41:39 WARN DAGScheduler: Broadcasting large task binary with size 1197.5 KiB\n",
      "24/02/23 08:41:41 WARN DAGScheduler: Broadcasting large task binary with size 1223.7 KiB\n"
     ]
    }
   ],
   "source": [
    "cv = CrossValidator(estimator=pipeline,\n",
    "                        evaluator=evaluator,\n",
    "                        estimatorParamMaps=paramGrid,\n",
    "                        numFolds=3,\n",
    "                        seed=42)\n",
    "cvModel = cv.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c140890d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({Param(parent='RandomForestRegressor_2249db9d114f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n",
       "   Param(parent='RandomForestRegressor_2249db9d114f', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  291.1822640924783),\n",
       " ({Param(parent='RandomForestRegressor_2249db9d114f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n",
       "   Param(parent='RandomForestRegressor_2249db9d114f', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  286.7714750274078),\n",
       " ({Param(parent='RandomForestRegressor_2249db9d114f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4,\n",
       "   Param(parent='RandomForestRegressor_2249db9d114f', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  287.6963245160818),\n",
       " ({Param(parent='RandomForestRegressor_2249db9d114f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4,\n",
       "   Param(parent='RandomForestRegressor_2249db9d114f', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  279.9927057236079),\n",
       " ({Param(parent='RandomForestRegressor_2249db9d114f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6,\n",
       "   Param(parent='RandomForestRegressor_2249db9d114f', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  294.34810870889305),\n",
       " ({Param(parent='RandomForestRegressor_2249db9d114f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6,\n",
       "   Param(parent='RandomForestRegressor_2249db9d114f', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  275.39862704729984)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da484314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 08:44:25 WARN BlockManager: Block rdd_1161_0 already exists on this machine; not re-adding it\n",
      "24/02/23 08:44:28 WARN DAGScheduler: Broadcasting large task binary with size 1323.1 KiB\n",
      "24/02/23 08:44:30 WARN DAGScheduler: Broadcasting large task binary with size 1156.6 KiB\n",
      "24/02/23 08:44:33 WARN DAGScheduler: Broadcasting large task binary with size 1197.5 KiB\n",
      "24/02/23 08:44:35 WARN DAGScheduler: Broadcasting large task binary with size 1223.7 KiB\n"
     ]
    }
   ],
   "source": [
    "cvModel = cv.setParallelism(4).fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c75fe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/23 08:46:07 WARN DAGScheduler: Broadcasting large task binary with size 1322.5 KiB\n",
      "24/02/23 08:46:09 WARN DAGScheduler: Broadcasting large task binary with size 1159.2 KiB\n",
      "24/02/23 08:46:11 WARN DAGScheduler: Broadcasting large task binary with size 1196.9 KiB\n",
      "24/02/23 08:46:13 WARN DAGScheduler: Broadcasting large task binary with size 1223.7 KiB\n"
     ]
    }
   ],
   "source": [
    "cv = CrossValidator(estimator=rf,\n",
    "                    evaluator=evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=3,\n",
    "                    parallelism=4,\n",
    "                    seed=42)\n",
    "pipeline = Pipeline(stages=[stringIndexer, vecAssembler, cv])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5006717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler \n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aea79a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnbDF = spark.read.parquet(filePath)\n",
    "(trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols,\n",
    "                                  outputCols=indexOutputCols,\n",
    "                                  handleInvalid=\"skip\")\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes\n",
    "if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs,\n",
    "                                   outputCol=\"features\")\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40, maxDepth=5,\n",
    "                               numTrees=100, seed=42)\n",
    "pipeline = Pipeline(stages=[stringIndexer, vecAssembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6c57f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "851811a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/anaconda3/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"random-forest\") as run: \n",
    "    # Log params: num_trees and max_depth \n",
    "    mlflow.log_param(\"num_trees\", rf.getNumTrees()) \n",
    "    mlflow.log_param(\"max_depth\", rf.getMaxDepth())\n",
    "    # Log model\n",
    "    pipelineModel = pipeline.fit(trainDF)\n",
    "    mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "    # Log metrics: RMSE and R2\n",
    "    predDF = pipelineModel.transform(testDF)\n",
    "    regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                              labelCol=\"price\")\n",
    "    rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
    "    r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "    mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n",
    "    # Log artifact: feature importance scores\n",
    "    rfModel = pipelineModel.stages[-1]\n",
    "    pandasDF = (pd.DataFrame(list(zip(vecAssembler.getInputCols(),\n",
    "                                        rfModel.featureImportances)),\n",
    "                               columns=[\"feature\", \"importance\"])\n",
    "                  .sort_values(by=\"importance\", ascending=False))\n",
    "# First write to local filesystem, then tell MLflow where to find that file\n",
    "pandasDF.to_csv(\"feature-importance.csv\", index=False)\n",
    "mlflow.log_artifact(\"feature-importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5270d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7852e916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "runs = client.search_runs(run.info.experiment_id,\n",
    "                              order_by=[\"attributes.start_time desc\"],\n",
    "                              max_results=1)\n",
    "run_id = runs[0].info.run_id\n",
    "runs[0].data.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0fae872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/23 14:27:59 INFO mlflow.projects.utils: === Fetching project from https://github.com/databricks/LearningSparkV2/#mlflow-project-example into /var/folders/4z/t2r1mqq10s59nz21h0y_16900000gr/T/tmpbb4dfdac ===\n",
      "2024/02/23 14:28:12 INFO mlflow.projects.utils: Fetched 'master' branch\n",
      "2024/02/23 14:28:15 INFO mlflow.utils.conda: === Creating conda environment mlflow-a91eb9b529409372aa4585b19c73952959a7a296 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ResolvePackageNotFound: \n",
      "  - python=3.7\n",
      "  - pandas=0.24\n",
      "  - pip=19.0.3\n",
      "\n"
     ]
    },
    {
     "ename": "ShellCommandException",
     "evalue": "Non-zero exit code: 1\nCommand: ['/Users/chongbei/anaconda3/bin/conda', 'env', 'create', '-n', 'mlflow-a91eb9b529409372aa4585b19c73952959a7a296', '--file', '/var/folders/4z/t2r1mqq10s59nz21h0y_16900000gr/T/tmpbb4dfdac/mlflow-project-example/conda.yaml', '--quiet']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mShellCommandException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m      2\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/databricks/LearningSparkV2/#mlflow-project-example\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m      parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_trees\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mlflow/projects/__init__.py:337\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(uri, entry_point, version, parameters, docker_args, experiment_name, experiment_id, backend, backend_config, storage_dir, synchronous, run_id, run_name, env_manager, build_image, docker_auth)\u001b[0m\n\u001b[1;32m    331\u001b[0m     backend_config_dict[MLFLOW_LOCAL_BACKEND_RUN_ID_CONFIG] \u001b[38;5;241m=\u001b[39m run_id\n\u001b[1;32m    333\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m _resolve_experiment_id(\n\u001b[1;32m    334\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name, experiment_id\u001b[38;5;241m=\u001b[39mexperiment_id\n\u001b[1;32m    335\u001b[0m )\n\u001b[0;32m--> 337\u001b[0m submitted_run_obj \u001b[38;5;241m=\u001b[39m _run(\n\u001b[1;32m    338\u001b[0m     uri\u001b[38;5;241m=\u001b[39muri,\n\u001b[1;32m    339\u001b[0m     experiment_id\u001b[38;5;241m=\u001b[39mexperiment_id,\n\u001b[1;32m    340\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39mentry_point,\n\u001b[1;32m    341\u001b[0m     version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    342\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    343\u001b[0m     docker_args\u001b[38;5;241m=\u001b[39mdocker_args,\n\u001b[1;32m    344\u001b[0m     backend_name\u001b[38;5;241m=\u001b[39mbackend,\n\u001b[1;32m    345\u001b[0m     backend_config\u001b[38;5;241m=\u001b[39mbackend_config_dict,\n\u001b[1;32m    346\u001b[0m     env_manager\u001b[38;5;241m=\u001b[39menv_manager,\n\u001b[1;32m    347\u001b[0m     storage_dir\u001b[38;5;241m=\u001b[39mstorage_dir,\n\u001b[1;32m    348\u001b[0m     synchronous\u001b[38;5;241m=\u001b[39msynchronous,\n\u001b[1;32m    349\u001b[0m     run_name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    350\u001b[0m     build_image\u001b[38;5;241m=\u001b[39mbuild_image,\n\u001b[1;32m    351\u001b[0m     docker_auth\u001b[38;5;241m=\u001b[39mdocker_auth,\n\u001b[1;32m    352\u001b[0m )\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synchronous:\n\u001b[1;32m    354\u001b[0m     _wait_for(submitted_run_obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mlflow/projects/__init__.py:106\u001b[0m, in \u001b[0;36m_run\u001b[0;34m(uri, experiment_id, entry_point, version, parameters, docker_args, backend_name, backend_config, storage_dir, env_manager, synchronous, run_name, build_image, docker_auth)\u001b[0m\n\u001b[1;32m    104\u001b[0m backend \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_backend(backend_name)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend:\n\u001b[0;32m--> 106\u001b[0m     submitted_run \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    107\u001b[0m         uri,\n\u001b[1;32m    108\u001b[0m         entry_point,\n\u001b[1;32m    109\u001b[0m         parameters,\n\u001b[1;32m    110\u001b[0m         version,\n\u001b[1;32m    111\u001b[0m         backend_config,\n\u001b[1;32m    112\u001b[0m         tracking_store_uri,\n\u001b[1;32m    113\u001b[0m         experiment_id,\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m     tracking\u001b[38;5;241m.\u001b[39mMlflowClient()\u001b[38;5;241m.\u001b[39mset_tag(\n\u001b[1;32m    116\u001b[0m         submitted_run\u001b[38;5;241m.\u001b[39mrun_id, MLFLOW_PROJECT_BACKEND, backend_name\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mlflow/projects/backend/local.py:171\u001b[0m, in \u001b[0;36mLocalBackend.run\u001b[0;34m(self, project_uri, entry_point, params, version, backend_config, tracking_uri, experiment_id)\u001b[0m\n\u001b[1;32m    169\u001b[0m     tracking\u001b[38;5;241m.\u001b[39mMlflowClient()\u001b[38;5;241m.\u001b[39mset_tag(active_run\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id, MLFLOW_PROJECT_ENV, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m     command_separator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m && \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 171\u001b[0m     conda_env \u001b[38;5;241m=\u001b[39m get_or_create_conda_env(project\u001b[38;5;241m.\u001b[39menv_config_path)\n\u001b[1;32m    172\u001b[0m     command_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m conda_env\u001b[38;5;241m.\u001b[39mget_activate_command()\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# In synchronous mode, run the entry point command in a blocking fashion, sending status\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# updates to the tracking server when finished. Note that the run state may not be\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# persisted to the tracking server if interrupted\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mlflow/utils/conda.py:283\u001b[0m, in \u001b[0;36mget_or_create_conda_env\u001b[0;34m(conda_env_path, env_id, capture_output, env_root_dir, pip_requirements_override)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     _create_conda_env_func \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# Retry conda env creation in a pytest session to avoid flaky test failures\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         _create_conda_env_retry\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTEST_CURRENT_TEST\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m _create_conda_env\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 283\u001b[0m     conda_env \u001b[38;5;241m=\u001b[39m _create_conda_env_func(\n\u001b[1;32m    284\u001b[0m         conda_env_path,\n\u001b[1;32m    285\u001b[0m         conda_env_create_path,\n\u001b[1;32m    286\u001b[0m         project_env_name,\n\u001b[1;32m    287\u001b[0m         conda_extra_env_vars,\n\u001b[1;32m    288\u001b[0m         capture_output,\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pip_requirements_override:\n\u001b[1;32m    292\u001b[0m         _logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstalling additional dependencies specified\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby pip_requirements_override: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpip_requirements_override\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mlflow/utils/conda.py:109\u001b[0m, in \u001b[0;36m_create_conda_env\u001b[0;34m(conda_env_path, conda_env_create_path, project_env_name, conda_extra_env_vars, capture_output)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_conda_env\u001b[39m(\n\u001b[1;32m    102\u001b[0m     conda_env_path,\n\u001b[1;32m    103\u001b[0m     conda_env_create_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     capture_output,\n\u001b[1;32m    107\u001b[0m ):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conda_env_path:\n\u001b[0;32m--> 109\u001b[0m         process\u001b[38;5;241m.\u001b[39m_exec_cmd(\n\u001b[1;32m    110\u001b[0m             [\n\u001b[1;32m    111\u001b[0m                 conda_env_create_path,\n\u001b[1;32m    112\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    114\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-n\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m                 project_env_name,\n\u001b[1;32m    116\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--file\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    117\u001b[0m                 conda_env_path,\n\u001b[1;32m    118\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--quiet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    119\u001b[0m             ],\n\u001b[1;32m    120\u001b[0m             extra_env\u001b[38;5;241m=\u001b[39mconda_extra_env_vars,\n\u001b[1;32m    121\u001b[0m             capture_output\u001b[38;5;241m=\u001b[39mcapture_output,\n\u001b[1;32m    122\u001b[0m         )\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m         process\u001b[38;5;241m.\u001b[39m_exec_cmd(\n\u001b[1;32m    125\u001b[0m             [\n\u001b[1;32m    126\u001b[0m                 conda_env_create_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m             capture_output\u001b[38;5;241m=\u001b[39mcapture_output,\n\u001b[1;32m    138\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mlflow/utils/process.py:120\u001b[0m, in \u001b[0;36m_exec_cmd\u001b[0;34m(cmd, throw_on_error, extra_env, capture_output, synchronous, stream_output, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m comp_process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mCompletedProcess(\n\u001b[1;32m    114\u001b[0m     process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    115\u001b[0m     returncode\u001b[38;5;241m=\u001b[39mreturncode,\n\u001b[1;32m    116\u001b[0m     stdout\u001b[38;5;241m=\u001b[39mstdout,\n\u001b[1;32m    117\u001b[0m     stderr\u001b[38;5;241m=\u001b[39mstderr,\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m throw_on_error \u001b[38;5;129;01mand\u001b[39;00m returncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ShellCommandException\u001b[38;5;241m.\u001b[39mfrom_completed_process(comp_process)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m comp_process\n",
      "\u001b[0;31mShellCommandException\u001b[0m: Non-zero exit code: 1\nCommand: ['/Users/chongbei/anaconda3/bin/conda', 'env', 'create', '-n', 'mlflow-a91eb9b529409372aa4585b19c73952959a7a296', '--file', '/var/folders/4z/t2r1mqq10s59nz21h0y_16900000gr/T/tmpbb4dfdac/mlflow-project-example/conda.yaml', '--quiet']"
     ]
    }
   ],
   "source": [
    " mlflow.run(\n",
    "      \"https://github.com/databricks/LearningSparkV2/#mlflow-project-example\",\n",
    "      parameters={\"max_depth\": 5, \"num_trees\": 100})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23258ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
